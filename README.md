# NLP-MINI-Project
# **FAQ Categorization**

## **Course: NLP (Semester 6\) \- Pillai College of Engineering**

### **Project Overview:**

This project is part of the **Natural Language Processing (NLP)** course for **Semester 6** students at **Pillai College of Engineering**. The project focuses on **FAQ Categorization**, where we apply various **Machine Learning (ML)**, **Deep Learning (DL)**, and **Language Models** to categorize FAQ into predefined categories. This project involves exploring techniques like text preprocessing, feature extraction, model training, and evaluating the models for their effectiveness in classifying FAQs.

You can learn more about the college by visiting the official website of [Pillai College of Engineering](https://www.pce.ac.in/).

### **Acknowledgements:**

We would like to express our sincere gratitude to the following individuals:

* **Theory Faculty**:  
  * **Dhiraj Amin**  
  * **Sharvari Govilkar**  
* **Lab Faculty**:  
  * **Dhiraj Amin**  
  * **Neha Ashok**  
  * **Shubhangi Chavan**

Their guidance and support have been invaluable throughout this project.

### **Project Title:**

**FAQ Categorization using Natural Language Processing**

### **Project Abstract:**

The **FAQ Categorization** project aims to classify FAQs into different categories like **Account Management, Technical Support, Payment Issues, General Inquiry, Security & Privacy**. This task involves applying **Machine Learning**, **Deep Learning**, and **Language Models** to accurately categorize FAQ text. The project explores different algorithms, including traditional machine learning techniques, deep learning models, and state-of-the-art pre-trained language models. The goal is to evaluate the performance of each approach and select the best-performing model for FAQ Categorization.

### **Algorithms Used:**

* **Machine Learning Algorithms**:

  * **Logistic Regression**  
  * **Support Vector Machine (SVM)**  
  * **Random Forest Classifier**  
* **Deep Learning Algorithms**:

  * **Convolutional Neural Networks (CNN)**  
  * **Bidirectional Long Short-Term Memory (BiLSTM)**  
  * **Long Short-Term Memory (LSTM)**  
* **Language Models**:

  * **RoBERTa**  
  * **BERT (Bidirectional Encoder Representations from Transformers)**

### **Comparative Analysis:**

The comparative analysis of different models highlights their effectiveness in classifying FAQs into the correct category. The following table summarizes theperformance of the models tested:

| Model Type      | Model               | Accuracy / MCC | Observations          |
| -----           | -----               | -----          | -----                 |
| **ML Models**   | Logistic Regression | 0.7833         | Best ML model         |
|                 | Random Forest       | 0.7192         | Moderate performance  |
|                 | SVM                 | 0.6798         | Lowest ML accuracy    |
| **DL Models**   | CNN                 | 0.7059         | Best among DL models  |
|                 | LSTM                | 0.6765         | Moderate performance  |
|                 | BiLSTM              | 0.6569         | Lowest DL accuracy    |
| **LLM Models**  | BERT (Fine-Tuned)   | 0.7376 (MCC)   | Improved performance  |
|                 | RoBERTa (Fine-Tuned)| 0.8169 (MCC)   | Best among all models |

**Conclusion:**

This **FAQ Categorization** project demonstrates the potential of **Machine Learning**, **Deep Learning**, and **Language Models** for text classification tasks, particularly for categorizing FAQs. The comparative analysis reveals that **BERT**, a transformer-based model, outperforms traditional methods and deep learning models in terms of **accuracy**, **precision**, and **recall**. By employing various algorithms, we gain insights into the strengths and weaknesses of each model, allowing us to choose the most suitable approach for FAQ Categorization.

---

This version tailors the description to focus on **FAQ Categorization** while maintaining the structure and content related to the NLP project. It includes relevant details on algorithms used, a comparative analysis table, and the necessary acknowledgments for faculty members involved in the course.

